{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7320bf1-d215-4378-89fd-3eedbdb5dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['sample', 'text']\n",
      "[]\n",
      "['contains', 'unnecessary', 'spaces']\n",
      "[]\n",
      "['Also', 'numbering', 'beginning', 'sentences']\n",
      "sample text contains unnecessary spaces also numbering beginning sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))  \n",
    "    sentences = sent_tokenize(text)\n",
    "    processed = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence, preserve_line=True)\n",
    "        # words = nltk.tokenize.casual_tokenize(sentence)\n",
    "\n",
    "\n",
    "        # Remove numbering at the start of the sentence\n",
    "        if words and words[0].isdigit():\n",
    "            words = words[1:]\n",
    "\n",
    "        # Remove stopwords\n",
    "        words = [word for word in words if word.lower() not in stop_words and word.lower() not in string.punctuation]\n",
    "        print(words)\n",
    "\n",
    "        # Reconstruct the sentence\n",
    "        if words:\n",
    "            cleaned_sentence = \" \".join(words)\n",
    "            processed.append(cleaned_sentence)\n",
    "\n",
    "    # Join sentences into a structured paragraph\n",
    "    normalized_text = \" \".join(processed).strip().lower()\n",
    "\n",
    "    return normalized_text\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"  1.   This    is a   sample   text.\\n \n",
    "           2. It  doesn't don't won't contains unnecessary spaces.   \n",
    "   3.  Also,   numbering at the beginning of sentences.   \"\"\"\n",
    "\n",
    "print(normalize_text(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ccb3ea6-fa79-4db5-a9e3-c755eddeb707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text banana banana bandana band bandit\n",
      "tokens ['b', 'a', 'n', 'a', 'n', 'a', ' ', 'b', 'a', 'n', 'a', 'n', 'a', ' ', 'b', 'a', 'n', 'd', 'a', 'n', 'a', ' ', 'b', 'a', 'n', 'd', ' ', 'b', 'a', 'n', 'd', 'i', 't']\n",
      "{('a', 'n'): 'an'}\n",
      "Merges: {('a', 'n'): 'an'}\n",
      "Encoded: [b'b', b'an', b'd', b'an', b'a']\n",
      "Decoded: bandana\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def learn_bpe(text, num_merges=50):\n",
    "    \"\"\"Learns BPE merges from input text.\"\"\"\n",
    "    text = text.encode('utf-8', errors='replace').decode('utf-8')  # Normalize text\n",
    "    tokens = list(text)  # Character-level tokenization\n",
    "    merges = {}\n",
    "\n",
    "    \n",
    "    print('text', text)\n",
    "    print(\"tokens\", tokens)\n",
    "\n",
    "    \n",
    "    for _ in range(num_merges):\n",
    "        bigrams = Counter(zip(tokens, tokens[1:]))\n",
    "        if not bigrams:\n",
    "            break\n",
    "        most_common = max(bigrams, key=bigrams.get)\n",
    "        new_token = ''.join(most_common)\n",
    "        merges[most_common] = new_token\n",
    "        tokens.append(new_token)  # Append new merged token\n",
    "    print(merges)\n",
    "    return merges\n",
    "\n",
    "def encode(text, merges):\n",
    "    \"\"\"Encodes text using BPE merges.\"\"\"\n",
    "    text = text.encode('utf-8', errors='replace').decode('utf-8')  # Normalize again\n",
    "    tokens = list(text)\n",
    "    \n",
    "    for bigram, new_token in merges.items():\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if (tokens[i], tokens[i + 1]) == bigram:\n",
    "                tokens[i] = new_token  # Merge into new token\n",
    "                del tokens[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return [t.encode('utf-8') for t in tokens]  # Convert to byte representation\n",
    "\n",
    "def decode(encoded_tokens, merges):\n",
    "    \"\"\"Decodes a sequence of byte tokens back to text.\"\"\"\n",
    "    tokens = [t.decode('utf-8') for t in encoded_tokens]  # Convert back to string\n",
    "    rev_merges = {v: k for k, v in merges.items()}  # Reverse merges\n",
    "\n",
    "    for new_token, bigram in rev_merges.items():\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] == new_token:\n",
    "                tokens[i:i+1] = bigram  # Expand back to original\n",
    "            i += 1\n",
    "\n",
    "    return ''.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "corpus = \"banana banana bandana band bandit\"\n",
    "merges = learn_bpe(corpus, num_merges=10)\n",
    "\n",
    "encoded = encode(\"bandana\", merges)\n",
    "decoded = decode(encoded, merges)\n",
    "\n",
    "print(\"Merges:\", merges)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7b9e8a6f-a538-4fa9-9f49-1b0ed5ba199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text b'banana banana bandana band bandit'\n",
      "text b'bandana'\n",
      "tokens [98, 97, 110, 100, 97, 110, 97]\n",
      "Merges: {(97, 110): 256, (98, (97, 110)): 257, (32, (98, (97, 110))): 258, ((97, 110), 97): 259, (((97, 110), 97), (32, (98, (97, 110)))): 260, ((((97, 110), 97), (32, (98, (97, 110)))), 100): 261, ((98, (97, 110)), (((97, 110), 97), (32, (98, (97, 110))))): 262, (((98, (97, 110)), (((97, 110), 97), (32, (98, (97, 110))))), ((((97, 110), 97), (32, (98, (97, 110)))), 100)): 263, ((((98, (97, 110)), (((97, 110), 97), (32, (98, (97, 110))))), ((((97, 110), 97), (32, (98, (97, 110)))), 100)), ((((97, 110), 97), (32, (98, (97, 110)))), 100)): 264, (((((98, (97, 110)), (((97, 110), 97), (32, (98, (97, 110))))), ((((97, 110), 97), (32, (98, (97, 110)))), 100)), ((((97, 110), 97), (32, (98, (97, 110)))), 100)), (32, (98, (97, 110)))): 265}\n",
      "Encoded: [98, 97, 110, 100, 97, 110, 97]\n",
      "Decoded: bandana\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "def learn_bpe(text, num_merges=50):\n",
    "    \"\"\"Learns BPE merges directly on byte-level encoding.\"\"\"\n",
    "    text = text.encode('utf-8', errors='replace')  # Convert text to bytes\n",
    "    tokens = list(text)  # Split into byte-level tokens\n",
    "    merges = {}\n",
    "    # print('text', text)\n",
    "    # print(\"tokens\", tokens\n",
    "    new_token_val = 256\n",
    "    for _ in range(num_merges):\n",
    "        bigrams = Counter(zip(tokens, tokens[1:]))\n",
    "        if not bigrams:\n",
    "            break\n",
    "        most_common = max(bigrams, key=bigrams.get)\n",
    "        # new_token = max(tokens) + 1  # Assign new byte value\n",
    "        new_token = new_token_val\n",
    "        merges[most_common] = new_token\n",
    "        i = 0\n",
    "        while i < len(tokens)-1:\n",
    "            if (tokens[i], tokens[i+1]) == most_common:\n",
    "                tokens[i] = most_common\n",
    "                del tokens[i+1]\n",
    "            else:\n",
    "                i+=1\n",
    "        new_token_val+=1\n",
    "\n",
    "    return merges\n",
    "\n",
    "def encode(text, merges):\n",
    "    \"\"\"Encodes text using learned BPE merges.\"\"\"\n",
    "    text = text.encode('utf-8', errors='replace')  # Convert to byte form\n",
    "    tokens = list(text)\n",
    "\n",
    "    print('text', text)\n",
    "    print(\"tokens\", tokens)\n",
    "\n",
    "    for bigram, new_token in merges.items():\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            if (tokens[i], tokens[i + 1]) == bigram:\n",
    "                tokens[i] = new_token  # Replace bigram with new token\n",
    "                del tokens[i + 1]\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "    return tokens  # Keep bytes as integers\n",
    "\n",
    "def decode(encoded_tokens, merges):\n",
    "    \"\"\"Decodes byte-tokenized text back to its original form.\"\"\"\n",
    "    rev_merges = {v: k for k, v in merges.items()}  # Reverse merges\n",
    "\n",
    "    i = 0\n",
    "    while i < len(encoded_tokens):\n",
    "        if encoded_tokens[i] in rev_merges:\n",
    "            encoded_tokens[i:i+1] = rev_merges[encoded_tokens[i]]  # Expand back\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return bytes(encoded_tokens).decode('utf-8', errors='replace')  # Convert back to text\n",
    "\n",
    "# Example usage\n",
    "corpus = \"banana banana bandana band bandit\"\n",
    "merges = learn_bpe(corpus, num_merges=10)\n",
    "\n",
    "encoded = encode(\"bandana\", merges)\n",
    "decoded = decode(encoded, merges)\n",
    "\n",
    "print(\"Merges:\", merges)\n",
    "print(\"Encoded:\", encoded)  # Byte-level encoding\n",
    "print(\"Decoded:\", decoded)  # Should return 'bandana'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb5f30d-a6f8-4e4d-aea4-696d601d678c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8572218-f129-4066-9591-5c942f6c52de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
